{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = 'MLApp'\n",
    "master = 'spark://spark-master:7077'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(master) \\\n",
    "                    .appName(appName) \\\n",
    "                    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "                    .config(\"spark.executor.core\", \"1\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- bedroom: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('hdfs://hadoop-namenode:8020/data', header=True)\n",
    "df = df.select('city', 'district', 'area (m^2)', 'bedroom', 'price (tỷ)')\n",
    "df = df.withColumnRenamed('area (m^2)', 'area') \\\n",
    "    .withColumnRenamed('price (tỷ)', 'price')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn('area', col('area').cast('double'))\n",
    "df = df.withColumn('price', col('price').cast('double'))\n",
    "df = df.withColumn('bedroom', col('bedroom').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- bedroom: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+-------+-------+\n",
      "|       city|    district| area|bedroom|  price|\n",
      "+-----------+------------+-----+-------+-------+\n",
      "|    Đà Nẵng|   Thanh Khê|93.58|    2.0|2.05876|\n",
      "|     Hà Nội| Nam Từ Liêm|225.0|    4.0|    0.0|\n",
      "|Hồ Chí Minh|      Quận 1|118.5|    3.0|   17.1|\n",
      "|   Hà Nội|      Tây Hồ| 70.0|    2.0|    3.2|\n",
      "|     Hà Nội| Nam Từ Liêm|145.0|    4.0|    0.0|\n",
      "|     Hà Nội|   Long Biên| 80.0|    3.0|    0.0|\n",
      "|Hồ Chí Minh|  Bình Thạnh| 83.0|    2.0|    4.1|\n",
      "| Bình Dương|    Thuận An| 72.0|    2.0|   1.89|\n",
      "|Hồ Chí Minh|  Bình Chánh| 65.0|    2.0|   1.97|\n",
      "|Hồ Chí Minh|      Quận 8| 78.0|    2.0|   2.45|\n",
      "|     Hà Nội| Nam Từ Liêm|135.0|    4.0|   5.67|\n",
      "|Hồ Chí Minh|      Quận 7|125.0|    3.0|    4.7|\n",
      "|Hồ Chí Minh|  Bình Thạnh|120.0|    3.0|    6.9|\n",
      "|Hồ Chí Minh|      Quận 7| 70.0|    2.0|   3.65|\n",
      "|     Hà Nội| Nam Từ Liêm| 88.0|    2.0|    4.3|\n",
      "|     Hà Nội|   Long Biên| 95.0|    3.0|    0.0|\n",
      "|     Hà Nội| Nam Từ Liêm|105.0|    2.0|   4.41|\n",
      "|     Hà Nội| Nam Từ Liêm|106.0|    3.0|    4.6|\n",
      "|Hồ Chí Minh|      Quận 7| 65.0|    2.0|   2.99|\n",
      "|     Hà Nội|     Gia Lâm| 68.0|    2.0|    0.0|\n",
      "|     Hà Nội|      Tây Hồ| 59.3|    2.0|    2.7|\n",
      "|     Hà Nội|     Đống Đa| 80.0|    2.0|    0.0|\n",
      "|Hồ Chí Minh|      Quận 1| 70.0|    2.0|    0.0|\n",
      "|     Hà Nội|   Long Biên| 74.0|    2.0|   2.22|\n",
      "|     Hà Nội|   Long Biên|100.0|    3.0|    0.0|\n",
      "| Bình Dương|       Dĩ An| 70.0|    2.0|    2.5|\n",
      "|     Hà Nội| Bắc Từ Liêm|107.0|    3.0|    4.4|\n",
      "|Hồ Chí Minh|     Tân Phú| 68.0|    2.0|    2.6|\n",
      "|Hồ Chí Minh|      Quận 2| 65.0|    2.0|    3.7|\n",
      "|Hồ Chí Minh|      Quận 2| 69.0|    2.0|    3.9|\n",
      "|Hồ Chí Minh|      Quận 9| 69.0|    2.0|   2.45|\n",
      "| Bình Dương|    Thuận An| 60.0|    2.0|    0.0|\n",
      "|     Hà Nội|      Tây Hồ| 72.0|    2.0|    2.5|\n",
      "| Bình Dương|    Thuận An| 38.0|    2.0|    0.0|\n",
      "|Hồ Chí Minh|      Quận 7|147.0|    3.0|    6.5|\n",
      "|     Hà Nội|   Long Biên|105.0|    3.0|    0.0|\n",
      "|     Hà Nội| Nam Từ Liêm|135.0|    3.0|    5.4|\n",
      "|     Hà Nội|     Gia Lâm| 55.6|    2.0|    2.4|\n",
      "|  Khánh Hòa|   Nha Trang| 35.0|    1.0|    1.5|\n",
      "|     Hà Nội|Hai Bà Trưng|105.0|    3.0|    0.0|\n",
      "|Hồ Chí Minh|  Bình Thạnh| 89.0|    2.0|   4.65|\n",
      "|     Hà Nội| Nam Từ Liêm| 63.0|    2.0|    2.3|\n",
      "|  Khánh Hòa|   Nha Trang| 61.0|    2.0|    0.0|\n",
      "|     Hà Nội| Bắc Từ Liêm|100.0|    3.0|   3.38|\n",
      "|     Hà Nội| Nam Từ Liêm|185.0|    4.0|   11.5|\n",
      "|Hồ Chí Minh|      Quận 7|120.0|    3.0|    5.0|\n",
      "|Hồ Chí Minh|      Quận 9| 53.0|    1.0|    2.6|\n",
      "|Hồ Chí Minh|  Bình Thạnh| 50.0|    1.0|    3.0|\n",
      "|     Hà Nội|     Gia Lâm| 80.0|    3.0|   2.69|\n",
      "|Hồ Chí Minh|  Bình Thạnh|120.0|    3.0|    7.8|\n",
      "+-----------+------------+-----+-------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.show(n=2, truncate=False, vertical=True)\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+-------+-------+\n",
      "|       city|   district| area|bedroom|  price|\n",
      "+-----------+-----------+-----+-------+-------+\n",
      "|    Đà Nẵng|  Thanh Khê|93.58|    2.0|2.05876|\n",
      "|Hồ Chí Minh|     Quận 1|118.5|    3.0|   17.1|\n",
      "|   Hà Nội|     Tây Hồ| 70.0|    2.0|    3.2|\n",
      "|Hồ Chí Minh| Bình Thạnh| 83.0|    2.0|    4.1|\n",
      "| Bình Dương|   Thuận An| 72.0|    2.0|   1.89|\n",
      "|Hồ Chí Minh| Bình Chánh| 65.0|    2.0|   1.97|\n",
      "|Hồ Chí Minh|     Quận 8| 78.0|    2.0|   2.45|\n",
      "|     Hà Nội|Nam Từ Liêm|135.0|    4.0|   5.67|\n",
      "|Hồ Chí Minh|     Quận 7|125.0|    3.0|    4.7|\n",
      "|Hồ Chí Minh| Bình Thạnh|120.0|    3.0|    6.9|\n",
      "|Hồ Chí Minh|     Quận 7| 70.0|    2.0|   3.65|\n",
      "|     Hà Nội|Nam Từ Liêm| 88.0|    2.0|    4.3|\n",
      "|     Hà Nội|Nam Từ Liêm|105.0|    2.0|   4.41|\n",
      "|     Hà Nội|Nam Từ Liêm|106.0|    3.0|    4.6|\n",
      "|Hồ Chí Minh|     Quận 7| 65.0|    2.0|   2.99|\n",
      "|     Hà Nội|     Tây Hồ| 59.3|    2.0|    2.7|\n",
      "|     Hà Nội|  Long Biên| 74.0|    2.0|   2.22|\n",
      "| Bình Dương|      Dĩ An| 70.0|    2.0|    2.5|\n",
      "|     Hà Nội|Bắc Từ Liêm|107.0|    3.0|    4.4|\n",
      "|Hồ Chí Minh|    Tân Phú| 68.0|    2.0|    2.6|\n",
      "|Hồ Chí Minh|     Quận 2| 65.0|    2.0|    3.7|\n",
      "|Hồ Chí Minh|     Quận 2| 69.0|    2.0|    3.9|\n",
      "|Hồ Chí Minh|     Quận 9| 69.0|    2.0|   2.45|\n",
      "|     Hà Nội|     Tây Hồ| 72.0|    2.0|    2.5|\n",
      "|Hồ Chí Minh|     Quận 7|147.0|    3.0|    6.5|\n",
      "|     Hà Nội|Nam Từ Liêm|135.0|    3.0|    5.4|\n",
      "|     Hà Nội|    Gia Lâm| 55.6|    2.0|    2.4|\n",
      "|  Khánh Hòa|  Nha Trang| 35.0|    1.0|    1.5|\n",
      "|Hồ Chí Minh| Bình Thạnh| 89.0|    2.0|   4.65|\n",
      "|     Hà Nội|Nam Từ Liêm| 63.0|    2.0|    2.3|\n",
      "|     Hà Nội|Bắc Từ Liêm|100.0|    3.0|   3.38|\n",
      "|     Hà Nội|Nam Từ Liêm|185.0|    4.0|   11.5|\n",
      "|Hồ Chí Minh|     Quận 7|120.0|    3.0|    5.0|\n",
      "|Hồ Chí Minh|     Quận 9| 53.0|    1.0|    2.6|\n",
      "|Hồ Chí Minh| Bình Thạnh| 50.0|    1.0|    3.0|\n",
      "|     Hà Nội|    Gia Lâm| 80.0|    3.0|   2.69|\n",
      "|Hồ Chí Minh| Bình Thạnh|120.0|    3.0|    7.8|\n",
      "|     Hà Nội|Nam Từ Liêm|106.0|    3.0|    4.6|\n",
      "|     Hà Nội|Bắc Từ Liêm| 99.0|    3.0|   3.85|\n",
      "| Bình Dương|   Thuận An| 72.0|    2.0|    1.8|\n",
      "|Hồ Chí Minh|     Quận 2| 70.0|    2.0|    4.0|\n",
      "|Hồ Chí Minh|     Quận 9| 47.0|    1.0|    1.7|\n",
      "|Hồ Chí Minh| Bình Thạnh| 50.0|    1.0|    3.1|\n",
      "|     Hà Nội|Nam Từ Liêm|145.0|    4.0|    5.2|\n",
      "|     Hà Nội|Nam Từ Liêm|115.0|    2.0|    9.8|\n",
      "|     Hà Nội|Bắc Từ Liêm| 85.0|    2.0|    3.2|\n",
      "|     Hà Nội|     Tây Hồ|168.0|    3.0|  5.208|\n",
      "|Hồ Chí Minh| Bình Thạnh|120.0|    3.0|    6.9|\n",
      "|     Hà Nội|Bắc Từ Liêm| 72.0|    2.0|   2.88|\n",
      "|Hồ Chí Minh|    Tân Phú| 65.0|    2.0|   2.45|\n",
      "+-----------+-----------+-----+-------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.filter(\"area>0\").filter(\"price>0\").filter(\"bedroom>0\").filter(\"area<500\").filter(\"price<20\")\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.regression import LinearRegression \n",
    "from pyspark.ml.feature import VectorIndexer \n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "def transData(dataset, categoricalCols, continuousCols, labelCol): \n",
    "    from pyspark.ml import Pipeline \n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler \n",
    "    from pyspark.sql.functions import col \n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_tndexed\".format(c)) \n",
    "                for c in categoricalCols ] \n",
    "    encoders = [ OneHotEncoder (inputCol=indexer.getOutputCol(), \n",
    "                                outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "                for indexer in indexers ] \n",
    "    assenbler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\") \n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assenbler]) \n",
    "    model=pipeline.fit(dataset) \n",
    "    data = model.transform(dataset) \n",
    "    data = data.withColumn('price',col(labelCol)) \n",
    "    return data.select('features','price','city', 'district', 'area', 'bedroom') \n",
    "categoricalCols= [\"city\", \"district\"] \n",
    "continuousCols = [ \"area\", \"bedroom\"] \n",
    "labelCol = \"price\"\n",
    "\n",
    "data= transData(df, categoricalCols, continuousCols, labelCol)\n",
    "data.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----+-----------+--------+----+-------+\n",
      "|                               features|price|       city|district|area|bedroom|\n",
      "+---------------------------------------+-----+-----------+--------+----+-------+\n",
      "| (121,[0,25,119,120],[1.0,1.0,3.0,2.0])| 2.77|Hồ Chí Minh|  Quận 2| 3.0|    2.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,30.6,1.0])| 2.26|Hồ Chí Minh|  Quận 2|30.6|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,33.0,1.0])|  1.6|Hồ Chí Minh|  Quận 2|33.0|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])| 1.92|Hồ Chí Minh|  Quận 2|34.0|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])| 1.92|Hồ Chí Minh|  Quận 2|34.0|    1.0|\n",
      "+---------------------------------------+-----+-----------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(trainingData.count())\n",
    "trainingData.show(5, trainingData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----+-----------+--------+----+-------+\n",
      "|                               features|price|       city|district|area|bedroom|\n",
      "+---------------------------------------+-----+-----------+--------+----+-------+\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])|  1.5|Hồ Chí Minh|  Quận 2|34.0|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,40.0,1.0])|  2.1|Hồ Chí Minh|  Quận 2|40.0|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,44.0,1.0])|  1.8|Hồ Chí Minh|  Quận 2|44.0|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,45.0,1.0])| 3.05|Hồ Chí Minh|  Quận 2|45.0|    1.0|\n",
      "|(121,[0,25,119,120],[1.0,1.0,48.5,1.0])|  2.4|Hồ Chí Minh|  Quận 2|48.5|    1.0|\n",
      "+---------------------------------------+-----+-----------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(testData.count())\n",
    "testData.show(5, testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 04:08:35 WARN Instrumentation: [c3ea3584] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/01/03 04:08:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/01/03 04:08:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/01/03 04:08:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/01/03 04:08:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "22/01/03 04:08:36 WARN Instrumentation: [c3ea3584] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "from pyspark.ml.regression import DecisionTreeRegressor \n",
    "from pyspark.ml.regression import RandomForestRegressor \n",
    "from pyspark.ml.regression import GBTRegressor \n",
    "#Linear Regression \n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\") \n",
    "lrModel = lr.fit(trainingData) \n",
    "#Decision Tree Regression \n",
    "dt = DecisionTreeRegressor (labelCol=\"price\", featuresCol=\"features\") \n",
    "dtModel = dt.fit(trainingData) \n",
    "#Random Forest Regression \n",
    "rf = RandomForestRegressor (labelCol=\"price\", featuresCol=\"features\") \n",
    "rfModel = rf.fit(trainingData) \n",
    "#Gradient-Boosted Tree Regression \n",
    "gbt = GBTRegressor (labelCol=\"price\", featuresCol=\"features\") \n",
    "gbtModel = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lrModel.transform(testData) \n",
    "dt_predictions = dtModel.transform(testData) \n",
    "rf_predictions = rfModel.transform(testData) \n",
    "gbt_predictions = gbtModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|                               features|       city|district|area|bedroom|price|        prediction|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])|Hồ Chí Minh|  Quận 2|34.0|    1.0|  1.5|3.3129353835951854|\n",
      "|(121,[0,25,119,120],[1.0,1.0,40.0,1.0])|Hồ Chí Minh|  Quận 2|40.0|    1.0|  2.1| 3.607181894057821|\n",
      "|(121,[0,25,119,120],[1.0,1.0,44.0,1.0])|Hồ Chí Minh|  Quận 2|44.0|    1.0|  1.8| 3.803346234366245|\n",
      "|(121,[0,25,119,120],[1.0,1.0,45.0,1.0])|Hồ Chí Minh|  Quận 2|45.0|    1.0| 3.05|  3.85238731944335|\n",
      "|(121,[0,25,119,120],[1.0,1.0,48.5,1.0])|Hồ Chí Minh|  Quận 2|48.5|    1.0|  2.4|  4.02403111721322|\n",
      "|(121,[0,25,119,120],[1.0,1.0,49.0,1.0])|Hồ Chí Minh|  Quận 2|49.0|    1.0|2.687| 4.048551659751774|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3| 4.097592744828879|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3| 4.097592744828879|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  1.9| 4.146633829905984|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  2.4| 4.146633829905984|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions.select('features','city', 'district', 'area', \n",
    "                      'bedroom','price', 'prediction').show(10, testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|                               features|       city|district|area|bedroom|price|        prediction|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])|Hồ Chí Minh|  Quận 2|34.0|    1.0|  1.5|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,40.0,1.0])|Hồ Chí Minh|  Quận 2|40.0|    1.0|  2.1|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,44.0,1.0])|Hồ Chí Minh|  Quận 2|44.0|    1.0|  1.8|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,45.0,1.0])|Hồ Chí Minh|  Quận 2|45.0|    1.0| 3.05|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,48.5,1.0])|Hồ Chí Minh|  Quận 2|48.5|    1.0|  2.4|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,49.0,1.0])|Hồ Chí Minh|  Quận 2|49.0|    1.0|2.687|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  1.9|2.4696012590299263|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  2.4|2.4696012590299263|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_predictions.select('features','city', 'district', 'area', \n",
    "                      'bedroom','price', 'prediction').show(10, testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|                               features|       city|district|area|bedroom|price|        prediction|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])|Hồ Chí Minh|  Quận 2|34.0|    1.0|  1.5|2.7504638325277915|\n",
      "|(121,[0,25,119,120],[1.0,1.0,40.0,1.0])|Hồ Chí Minh|  Quận 2|40.0|    1.0|  2.1|2.7604027214166806|\n",
      "|(121,[0,25,119,120],[1.0,1.0,44.0,1.0])|Hồ Chí Minh|  Quận 2|44.0|    1.0|  1.8|2.7604027214166806|\n",
      "|(121,[0,25,119,120],[1.0,1.0,45.0,1.0])|Hồ Chí Minh|  Quận 2|45.0|    1.0| 3.05|2.7604027214166806|\n",
      "|(121,[0,25,119,120],[1.0,1.0,48.5,1.0])|Hồ Chí Minh|  Quận 2|48.5|    1.0|  2.4|2.9493308276825316|\n",
      "|(121,[0,25,119,120],[1.0,1.0,49.0,1.0])|Hồ Chí Minh|  Quận 2|49.0|    1.0|2.687|2.9493308276825316|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3|2.9493308276825316|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3|2.9493308276825316|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  1.9|2.9227965955749795|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  2.4|2.9227965955749795|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.select('features','city', 'district', 'area', \n",
    "                      'bedroom','price', 'prediction').show(10, testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|                               features|       city|district|area|bedroom|price|        prediction|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "|(121,[0,25,119,120],[1.0,1.0,34.0,1.0])|Hồ Chí Minh|  Quận 2|34.0|    1.0|  1.5|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,40.0,1.0])|Hồ Chí Minh|  Quận 2|40.0|    1.0|  2.1|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,44.0,1.0])|Hồ Chí Minh|  Quận 2|44.0|    1.0|  1.8|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,45.0,1.0])|Hồ Chí Minh|  Quận 2|45.0|    1.0| 3.05|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,48.5,1.0])|Hồ Chí Minh|  Quận 2|48.5|    1.0|  2.4|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,49.0,1.0])|Hồ Chí Minh|  Quận 2|49.0|    1.0|2.687|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,50.0,1.0])|Hồ Chí Minh|  Quận 2|50.0|    1.0|  7.3|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  1.9|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|  2.4|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0| 2.58|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,51.0,1.0])|Hồ Chí Minh|  Quận 2|51.0|    1.0|2.799|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,55.0,1.0])|Hồ Chí Minh|  Quận 2|55.0|    1.0|  2.2|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,55.0,2.0])|Hồ Chí Minh|  Quận 2|55.0|    2.0|  1.6|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,55.0,2.0])|Hồ Chí Minh|  Quận 2|55.0|    2.0| 1.75|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,56.0,1.0])|Hồ Chí Minh|  Quận 2|56.0|    1.0|  4.0|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,58.0,1.0])|Hồ Chí Minh|  Quận 2|58.0|    1.0| 3.99|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,58.0,2.0])|Hồ Chí Minh|  Quận 2|58.0|    2.0|  4.0|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,60.0,1.0])|Hồ Chí Minh|  Quận 2|60.0|    1.0|  5.0|2.3988657017210455|\n",
      "|(121,[0,25,119,120],[1.0,1.0,61.0,2.0])|Hồ Chí Minh|  Quận 2|61.0|    2.0|  3.8|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,62.0,1.0])|Hồ Chí Minh|  Quận 2|62.0|    1.0|  4.0|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,63.0,1.0])|Hồ Chí Minh|  Quận 2|63.0|    1.0|  7.5|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,63.0,2.0])|Hồ Chí Minh|  Quận 2|63.0|    2.0|  3.2|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,64.0,2.0])|Hồ Chí Minh|  Quận 2|64.0|    2.0|  3.1|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,64.0,2.0])|Hồ Chí Minh|  Quận 2|64.0|    2.0|  3.9|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,65.0,2.0])|Hồ Chí Minh|  Quận 2|65.0|    2.0|  1.8|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,68.0,2.0])|Hồ Chí Minh|  Quận 2|68.0|    2.0| 2.23|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,68.0,2.0])|Hồ Chí Minh|  Quận 2|68.0|    2.0| 2.23|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,69.0,2.0])|Hồ Chí Minh|  Quận 2|69.0|    2.0|  3.7|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,69.0,2.0])|Hồ Chí Minh|  Quận 2|69.0|    2.0|  3.8|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,69.0,2.0])|Hồ Chí Minh|  Quận 2|69.0|    2.0|  3.9|2.5160758571119937|\n",
      "|(121,[0,25,119,120],[1.0,1.0,70.0,2.0])|Hồ Chí Minh|  Quận 2|70.0|    2.0|  4.0| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,70.0,2.0])|Hồ Chí Minh|  Quận 2|70.0|    2.0|  4.5| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,70.0,2.0])|Hồ Chí Minh|  Quận 2|70.0|    2.0|  4.6| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,70.4,2.0])|Hồ Chí Minh|  Quận 2|70.4|    2.0|  4.5| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,72.0,2.0])|Hồ Chí Minh|  Quận 2|72.0|    2.0| 5.95| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,73.0,2.0])|Hồ Chí Minh|  Quận 2|73.0|    2.0| 4.31| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,73.0,2.0])|Hồ Chí Minh|  Quận 2|73.0|    2.0|4.379| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,74.0,2.0])|Hồ Chí Minh|  Quận 2|74.0|    2.0|  2.9| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,76.0,2.0])|Hồ Chí Minh|  Quận 2|76.0|    2.0| 4.25| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,78.0,2.0])|Hồ Chí Minh|  Quận 2|78.0|    2.0|  3.2| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,78.0,2.0])|Hồ Chí Minh|  Quận 2|78.0|    2.0| 3.25| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,79.0,2.0])|Hồ Chí Minh|  Quận 2|79.0|    2.0|  4.5| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,79.0,2.0])|Hồ Chí Minh|  Quận 2|79.0|    2.0|  5.7| 4.019846090226927|\n",
      "|(121,[0,25,119,120],[1.0,1.0,79.0,3.0])|Hồ Chí Minh|  Quận 2|79.0|    3.0|  4.3|3.9885272837963264|\n",
      "|(121,[0,25,119,120],[1.0,1.0,80.0,2.0])|Hồ Chí Minh|  Quận 2|80.0|    2.0| 4.09| 4.188343908850007|\n",
      "|(121,[0,25,119,120],[1.0,1.0,80.0,2.0])|Hồ Chí Minh|  Quận 2|80.0|    2.0|  6.4| 4.188343908850007|\n",
      "|(121,[0,25,119,120],[1.0,1.0,80.0,3.0])|Hồ Chí Minh|  Quận 2|80.0|    3.0| 3.25| 4.157025102419407|\n",
      "|(121,[0,25,119,120],[1.0,1.0,80.0,3.0])|Hồ Chí Minh|  Quận 2|80.0|    3.0|  3.6| 4.157025102419407|\n",
      "|(121,[0,25,119,120],[1.0,1.0,81.0,2.0])|Hồ Chí Minh|  Quận 2|81.0|    2.0|  6.5| 4.188343908850007|\n",
      "+---------------------------------------+-----------+--------+----+-------+-----+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_predictions.select('features','city', 'district', 'area', \n",
    "                      'bedroom','price', 'prediction').show(50, testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "eval1 = RegressionEvaluator( \n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\") \n",
    "lr_rmse = float(eval1.evaluate(lr_predictions)) \n",
    "dt_rmse = float(eval1.evaluate(dt_predictions)) \n",
    "rf_rmse = float(eval1.evaluate(rf_predictions)) \n",
    "gbt_rmse = float(eval1.evaluate(gbt_predictions)) \n",
    "\n",
    "eval2 = RegressionEvaluator( \n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\") \n",
    "lr_mae = float(eval2.evaluate(lr_predictions)) \n",
    "dt_mae = float(eval2.evaluate(dt_predictions)) \n",
    "rf_mae = float(eval2.evaluate(rf_predictions)) \n",
    "gbt_mae = float(eval2.evaluate(gbt_predictions)) \n",
    "\n",
    "num = float(testData.count()) \n",
    "\n",
    "lr_rate = lr_predictions.filter(\"prediction/price < 1.3 and prediction/price > 0.7\").count()/num*100 \n",
    "dt_rate = dt_predictions.filter(\"prediction/price < 1.3 and prediction/price > 0.7\").count()/num*100\n",
    "rf_rate = rf_predictions.filter(\"prediction/price < 1.3 and prediction/price > 0.7\").count()/num*100  \n",
    "gbt_rate = gbt_predictions.filter(\"prediction/price < 1.3 and prediction/price > 0.7\").count()/num*100 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = spark.createDataFrame( \n",
    "    [\n",
    "        (\"Linear Regression\", lr_rmse, lr_mae, lr_rate), \n",
    "        (\"Decision Tree Regression\", dt_rmse, dt_mae, dt_rate), \n",
    "        (\"Random Forest Regression\", rf_rmse, rf_mae, rf_rate), \n",
    "        (\"Gradient-Boosted Tree Regression\", gbt_rmse, gbt_mae, gbt_rate), \n",
    "    ],\n",
    "    [\"Algorithms\", 'Root mean squared error', \"Mean absolute error\", \"Accuracy Rate (%)\"] \n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 04:12:47 WARN TaskSetManager: Lost task 0.0 in stage 284.0 (TID 290) (172.18.0.6 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/01/03 04:12:47 ERROR TaskSetManager: Task 0 in stage 284.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o992.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 284.0 failed 4 times, most recent failure: Lost task 0.3 in stage 284.0 (TID 293) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1856/2077401406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o992.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 284.0 failed 4 times, most recent failure: Lost task 0.3 in stage 284.0 (TID 293) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/bin/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "evaluation.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
